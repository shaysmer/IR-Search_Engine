{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE  SCHEDULED_STOP\r\n","cluster-0016  GCE       2                                             RUNNING  us-central1-a\r\n","\r\n","\r\n","To take a quick anonymous survey, run:\r\n","  $ gcloud survey\r\n","\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security → Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Dec  8 19:12 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-0016-m.us-central1-a.c.ferrous-module-480122-e7.internal:35845\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7feea355ee80>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":11,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parquet files: 60\n","Sample paths: ['gs://shay-208886382-bucket/multistream10_preprocessed.parquet', 'gs://shay-208886382-bucket/multistream11_part2_preprocessed.parquet', 'gs://shay-208886382-bucket/multistream11_preprocessed.parquet', 'gs://shay-208886382-bucket/multistream12_part2_preprocessed.parquet', 'gs://shay-208886382-bucket/multistream12_preprocessed.parquet']\n"]}],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'shay-208886382-bucket'\n","full_path = f\"gs://{bucket_name}/\"\n","paths = []\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","\n","for b in blobs:\n","    # keep ONLY parquet files (critical)\n","    if b.name.endswith(\".parquet\"):\n","        paths.append(full_path + b.name)\n","\n","print(\"Number of parquet files:\", len(paths))\n","print(\"Sample paths:\", paths[:5])\n"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"13ZX4ervQkku"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"02f81c72"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":13,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n"]},{"cell_type":"code","execution_count":14,"id":"2f5412ca","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["uploaded doc_title_dic.pickle, size: 6348910\n"]}],"source":["\n","# 1) build doc_id -> title dictionary\n","doc_title_dic = (\n","    parquetFile\n","    .select(\"id\", \"title\")\n","    .rdd\n","    .map(lambda r: (int(r[\"id\"]), r[\"title\"]))\n","    .collectAsMap()\n",")\n","\n","# 2) save locally\n","with open(\"doc_title_dic.pickle\", \"wb\") as f:\n","    pickle.dump(doc_title_dic, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# 3) upload to GCS bucket root\n","client = storage.Client()\n","bucket = client.bucket(bucket_name)\n","bucket.blob(\"doc_title_dic.pickle\").upload_from_filename(\"doc_title_dic.pickle\")\n","\n","print(\"uploaded doc_title_dic.pickle, size:\", len(doc_title_dic))\n"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"f6375562"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":9,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":10,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":11,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":12,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"72bcf46a"},"source":["**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n","\n","A few notes: \n","1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n","2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n","3. You are not allowed to change any of the code not coming from Colab. "]},{"cell_type":"code","execution_count":19,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","\n","def token2bucket_id(token):\n","    return int(_hash(token), 16) % NUM_BUCKETS\n","\n","from collections import Counter\n","from inverted_index_gcp import InvertedIndex\n","\n","\n","def word_count(text, doc_id):\n","    if text is None:\n","        return []\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    tokens = [m.group() for m in RE_WORD.finditer(text.lower())]\n","    tokens = [t for t in tokens if t not in all_stopwords]\n","\n","    w2cnt = Counter(tokens)\n","    return [(w, (doc_id, tf)) for w, tf in w2cnt.items()]\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","    # unsorted_pl: iterable of (doc_id, tf)\n","    return sorted(unsorted_pl, key=lambda x: x[0])\n","\n","\n","def calculate_df(postings_rdd):\n","    # postings_rdd: (term, posting_list)\n","    return postings_rdd.map(lambda x: (x[0], len(x[1])))\n","\n","\n","def partition_postings_and_write(postings_rdd):\n","    # postings_rdd: (term, posting_list)\n","    buckets = postings_rdd.map(lambda x: (token2bucket_id(x[0]), x))\n","    grouped = buckets.groupByKey()\n","\n","    def write_bucket(bucket_tuple):\n","        bucket_id, iterable_items = bucket_tuple\n","        list_w_pl = list(iterable_items)\n","        b_w_pl = (bucket_id, list_w_pl)\n","        posting_locs = InvertedIndex.write_a_posting_list(b_w_pl, bucket_name)\n","        return posting_locs\n","\n","    return grouped.map(write_bucket)\n"]},{"cell_type":"code","execution_count":null,"id":"f57e1e57","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":21,"id":"3dbc0e14","metadata":{"id":"348pECY8cH-T","nbgrader":{"grade":true,"grade_id":"cell-index_const_time","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# test index construction time\n","assert index_const_time < 60*120"]},{"cell_type":"code","execution_count":2,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[{"ename":"NameError","evalue":"name 'defaultdict' is not defined","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# collect all posting lists locations into one super-set\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m super_posting_locs \u001B[38;5;241m=\u001B[39m \u001B[43mdefaultdict\u001B[49m(\u001B[38;5;28mlist\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blob \u001B[38;5;129;01min\u001B[39;00m client\u001B[38;5;241m.\u001B[39mlist_blobs(bucket_name, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpostings_gcp\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m      4\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m blob\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickle\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n","\u001B[0;31mNameError\u001B[0m: name 'defaultdict' is not defined"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":1,"id":"7d0a77d0","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'w2df_dict' is not defined","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m N_docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m6348910\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# 2. חישוב IDF לכל מילה (נשתמש ב-w2df_dict שיצרת בתא 20)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# log10(N/df) הוא הסטנדרט המקובל\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m idf_dict \u001B[38;5;241m=\u001B[39m {word: math\u001B[38;5;241m.\u001B[39mlog10(N_docs \u001B[38;5;241m/\u001B[39m df) \u001B[38;5;28;01mfor\u001B[39;00m word, df \u001B[38;5;129;01min\u001B[39;00m \u001B[43mw2df_dict\u001B[49m\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# 3. חישוב נורמה לכל מסמך (doc_norm) לטובת קוסינוס סימילריטי\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# עוברים על ה-postings_filtered, מחשבים (tf * idf)^2 ומחברים לכל מסמך\u001B[39;00m\n\u001B[1;32m     12\u001B[0m doc_norms \u001B[38;5;241m=\u001B[39m postings_filtered\u001B[38;5;241m.\u001B[39mflatMap(\u001B[38;5;28;01mlambda\u001B[39;00m x: [(doc_id, (tf \u001B[38;5;241m*\u001B[39m idf_dict\u001B[38;5;241m.\u001B[39mget(x[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0\u001B[39m))\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m doc_id, tf \u001B[38;5;129;01min\u001B[39;00m x[\u001B[38;5;241m1\u001B[39m]]) \\\n\u001B[1;32m     13\u001B[0m                              \u001B[38;5;241m.\u001B[39mreduceByKey(\u001B[38;5;28;01mlambda\u001B[39;00m a, b: a \u001B[38;5;241m+\u001B[39m b) \\\n\u001B[1;32m     14\u001B[0m                              \u001B[38;5;241m.\u001B[39mmapValues(\u001B[38;5;28;01mlambda\u001B[39;00m s: math\u001B[38;5;241m.\u001B[39msqrt(s))\n","\u001B[0;31mNameError\u001B[0m: name 'w2df_dict' is not defined"]}],"source":["import math\n","\n","# 1. הגדרת N (מספר הדפים שספרת בתא 9)\n","N_docs = 6348910\n","\n","# 2. חישוב IDF לכל מילה (נשתמש ב-w2df_dict שיצרת בתא 20)\n","# log10(N/df) הוא הסטנדרט המקובל\n","idf_dict = {word: math.log10(N_docs / df) for word, df in w2df_dict.items()}\n","\n","# 3. חישוב נורמה לכל מסמך (doc_norm) לטובת קוסינוס סימילריטי\n","# עוברים על ה-postings_filtered, מחשבים (tf * idf)^2 ומחברים לכל מסמך\n","doc_norms = postings_filtered.flatMap(lambda x: [(doc_id, (tf * idf_dict.get(x[0], 0))**2) for doc_id, tf in x[1]]) \\\n","                             .reduceByKey(lambda a, b: a + b) \\\n","                             .mapValues(lambda s: math.sqrt(s))\n","\n","# 4. חישוב אורך כל מסמך (doc_len)\n","# נשתמש ב-doc_text_pairs מתא 8 ובפונקציית ה-word_count מתא 19\n","doc_lens = doc_text_pairs.map(lambda x: (x[1], len(word_count(x[0], x[1]))))\n","\n","# 5. איחוד לתוך מילון weights: {doc_id: (len, norm)}\n","full_weights_dict = doc_lens.leftOuterJoin(doc_norms) \\\n","                            .mapValues(lambda x: (x[0], x[1] if x[1] is not None else 0.0)) \\\n","                            .collectAsMap()"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"VhAV0A6dNZWY"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":23,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 18.4 MiB/ 18.4 MiB]                                                \n","Operation completed over 1 objects/18.4 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","\n","inverted.N = N_docs\n","inverted.weights = full_weights_dict\n","\n","# write the global stats out\n","inverted.write_index('.', 'index')\n","# upload to gs\n","index_src = \"index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":24,"id":"8f880d59","metadata":{"id":"msogGbJ3c8JF","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":[" 18.45 MiB  2025-12-09T13:22:30Z  gs://shay-208886382-bucket/postings_gcp/index.pkl\r\n","TOTAL: 1 objects, 19342504 bytes (18.45 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"fc0667a9","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# PageRank"]},{"cell_type":"markdown","id":"0875c6bd","metadata":{"id":"fdd1bdca","nbgrader":{"grade":false,"grade_id":"cell-2fee4bc8d83c1e2a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["**YOUR TASK (10 POINTS):** Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below."]},{"cell_type":"code","execution_count":25,"id":"31a516e2","metadata":{"id":"yVjnTvQsegc-"},"outputs":[],"source":["def generate_graph(pages):\n","  edges = (\n","        pages.flatMap(\n","            lambda row: [\n","                (row.id, anchor[0])       \n","                for anchor in (row.anchor_text or [])\n","                if anchor is not None and anchor[0] is not None\n","            ]\n","        )\n","        .distinct()\n","    )\n","\n","\n","  vertices = (\n","        edges\n","        .flatMap(lambda e: e)     \n","        .distinct()\n","        .map(lambda vid: (vid,))  \n","    )\n","  return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"6bc05ba3","metadata":{"id":"db005700","nbgrader":{"grade":false,"grade_id":"cell-PageRank","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 345:==============================================>      (174 + 8) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|     id|          pagerank|\n","+-------+------------------+\n","|3434750| 9913.728782160775|\n","|  10568|  5385.34926364204|\n","|  32927| 5282.081575765277|\n","|  30680| 5128.233709604118|\n","|5843419| 4957.567686263866|\n","|  68253|4769.2782653551585|\n","|  31717| 4486.350180548309|\n","|  11867|  4146.41465091277|\n","|  14533| 3996.466440885503|\n","| 645042|3531.6270898037437|\n","|  17867| 3246.098390604142|\n","|5042916|2991.9457391661786|\n","|4689264| 2982.324883041747|\n","|  14532|2934.7468292031695|\n","|  25391| 2903.546223513398|\n","|   5405|2891.4163291546365|\n","|4764461|2834.3669873326617|\n","|  15573| 2783.865118158838|\n","|   9316| 2782.039646413769|\n","|8569916|2775.2861918400163|\n","+-------+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["t_start = time()\n","pages_links = (\n","    spark.read\n","         .parquet(f\"gs://{bucket_name}/multistream*\")\n","         .select(\"id\", \"anchor_text\")\n","         .rdd\n",")\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr_time = time() - t_start\n","pr.show()"]},{"cell_type":"code","execution_count":27,"id":"f7717604","metadata":{"id":"2cc36ca9","nbgrader":{"grade":true,"grade_id":"cell-PageRank_time","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# test that PageRank computaion took less than 1 hour\n","assert pr_time < 60*120"]},{"cell_type":"markdown","id":"96e9a610","metadata":{"id":"7f39m5R5TzZ2"},"source":["# Reporting"]},{"cell_type":"markdown","id":"a1da57c7","metadata":{"id":"HDMJxXTFT4YU"},"source":["**YOUR TASK (5 points):** execute and complete the following lines to complete \n","the reporting requirements for assignment #3. "]},{"cell_type":"code","execution_count":28,"id":"0f0d5523","metadata":{"id":"a0ec9661","nbgrader":{"grade":false,"grade_id":"cell-size_ofi_input_data","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"54595c29-4ae3-4b78-86d0-d8457ae9c150"},"outputs":[{"name":"stdout","output_type":"stream","text":["14.28 GiB    gs://wikidata_preprocessed\r\n"]}],"source":["# size of input data\n","!gsutil du -sh \"gs://wikidata_preprocessed/\""]},{"cell_type":"code","execution_count":29,"id":"ce25a98a","metadata":{"id":"264e0792","nbgrader":{"grade":false,"grade_id":"cell-size_of_index_data","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"44d9721a-1cd7-4e59-9f78-5439864cfdad"},"outputs":[{"name":"stdout","output_type":"stream","text":["5.92 GiB     gs://shay-208886382-bucket/postings_gcp\r\n"]}],"source":["# size of index data\n","index_dst = f'gs://{bucket_name}/postings_gcp/'\n","!gsutil du -sh \"$index_dst\""]},{"cell_type":"code","execution_count":31,"id":"7a9538ee","metadata":{"id":"LQ7r5rxvVuXb","nbgrader":{"grade":false,"grade_id":"cell-credits","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["I used 2.52 USD credit during the course of this assignment\n"]}],"source":["# How many USD credits did you use in GCP during the course of this assignment?\n","cost = 2.54 \n","print(f'I used {cost} USD credit during the course of this assignment')"]},{"cell_type":"markdown","id":"fb0e0ed8","metadata":{"id":"fdd1bdca"},"source":["**Bonus (10 points)** if you implement PageRank in pure PySpark, i.e. without using the GraphFrames package, AND manage to complete 10 iterations of your algorithm on the entire English Wikipedia in less than an hour. \n"]},{"cell_type":"code","execution_count":null,"id":"b8157868","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-PageRank_Bonus","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_40 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_30 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_53 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_42 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_1 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_22 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_36 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_115 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_83 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_5 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_94 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_9 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_108 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_53 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_27 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_57 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_42 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_46 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_9 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_108 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_57 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_73 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_107 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_106_119 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_22 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_101 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_65 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_77 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_117 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_64 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_46 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_106_121 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_90 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_40 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_58 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_25 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_42 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_34 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_36 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_103 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_94 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_64 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_46 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_33 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_108 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_53 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_81 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_17 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_42 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_78 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_57 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_83 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_83 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_92_119 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_81 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_107 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_14 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_108 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_112 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_1 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_17 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_72 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_40 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_33 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_40 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_78 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_101 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_53 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_25 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_85 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_33 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_115 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_96_121 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_68 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_64 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_85 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_57 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_5 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_1 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_22 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_81 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_46 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_83 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_30 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_117 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_117 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_73 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_25 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_94 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_17 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_50 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_14 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_101 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_40 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_33 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_90 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_1 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_31 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_85 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_9 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_107 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_5 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_94 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_9 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_30 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_58 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_42 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_68 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_68 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_85 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_83 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_108 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_25 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_115 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_58 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_192_119 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_30 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_192_121 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_1 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_30 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_2 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_96_119 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_56 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_104 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_17 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_73 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_103 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_88 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_101 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_36 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_85 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_73 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_57 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_33 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_92_121 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_38 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_59 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_94 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_118 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_94 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_103 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_14 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_101 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_24 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_41 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_96 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_25 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_14 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_36 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_5 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_81 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_68 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_107 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_68 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_117 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_90 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_11 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_22 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_64 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_103 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_68 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_64 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_18 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_85 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_53 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_14 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_58 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_107 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_78 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_10 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_46 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_36 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_90 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_90 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_22 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_17 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_103 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_58 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_9 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_46 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_5 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_78 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_109 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_78 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_117 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_81 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_115 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_6 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_81 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_115 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_17 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_100 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_120 !\n","25/12/09 15:30:35 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_73 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_20 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_112 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_62 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_192_106 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_28 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_70 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_56 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_86 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_116 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_101 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_52 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_33 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_97 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_7 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_78 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_45 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_20 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_107 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_116 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_56 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_26 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_69 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_35 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_57 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_96_118 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_75 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_75 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_39 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_25 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_69 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_45 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_26 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_98 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_60 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_11 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_42 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_97 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_16 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_5 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_49 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_20 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_86 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_123 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_192_118 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_49 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_34 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_18 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_105 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_7 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_52 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_49 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_61 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_3 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_39 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_26 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_97 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_70 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_8 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_64 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_106_118 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_49 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_106_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_3 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_98 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_3 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_7 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_105 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_12 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_11 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_89 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_26 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_89 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_28 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_12 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_70 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_52 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_120 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_116 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_52 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_120 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_49 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_97 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_112 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_96 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_97 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_3 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_12 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_98 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_45 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_96 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_74 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_96 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_75 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_9 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_20 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_120 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_115 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_98 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_96 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_74 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_45 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_12 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_123 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_7 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_86 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_34 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_61 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_70 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_75 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_123 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_86 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_34 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_26 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_86 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_7 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_74 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_98 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_80 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_70 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_19 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_62 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_39 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_74 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_34 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_11 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_49 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_75 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_70 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_96 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_61 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_123 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_18 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_69 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_92_118 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_61 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_12 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_105 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_123 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_62 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_86 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_120 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_28 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_45 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_18 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_111 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_39 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_116 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_18 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_105 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_105 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_96_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_20 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_56 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_62 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_112 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_56 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_93 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_89 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_116 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_34 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_69 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_112 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_39 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_61 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_192_113 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_18 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_89 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_11 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_104_89 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_62 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_26 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_11 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_74 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_69 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_53 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_52 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_97 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_112 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_119 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_74 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_43 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_265_1 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_28 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_186_28 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_196_120 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_56 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_94_89 !\n","25/12/09 15:30:36 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_89_3 !\n","25/12/09 15:30:37 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1765270135351_0007_01_000014 on host: cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 143. Diagnostics: [2025-12-09 15:30:36.777]Container killed on request. Exit code is 143\n","[2025-12-09 15:30:36.777]Container exited with a non-zero exit code 143. \n","[2025-12-09 15:30:36.778]Killed by external signal\n",".\n","25/12/09 15:30:37 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 14 on cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal: Container from a bad node: container_1765270135351_0007_01_000014 on host: cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 143. Diagnostics: [2025-12-09 15:30:36.777]Container killed on request. Exit code is 143\n","[2025-12-09 15:30:36.777]Container exited with a non-zero exit code 143. \n","[2025-12-09 15:30:36.778]Killed by external signal\n",".\n","25/12/09 15:30:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 354.0 (TID 6118) (cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765270135351_0007_01_000014 on host: cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 143. Diagnostics: [2025-12-09 15:30:36.777]Container killed on request. Exit code is 143\n","[2025-12-09 15:30:36.777]Container exited with a non-zero exit code 143. \n","[2025-12-09 15:30:36.778]Killed by external signal\n",".\n","25/12/09 15:30:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.0 in stage 354.0 (TID 6122) (cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765270135351_0007_01_000014 on host: cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 143. Diagnostics: [2025-12-09 15:30:36.777]Container killed on request. Exit code is 143\n","[2025-12-09 15:30:36.777]Container exited with a non-zero exit code 143. \n","[2025-12-09 15:30:36.778]Killed by external signal\n",".\n","25/12/09 15:30:37 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 14 for reason Container from a bad node: container_1765270135351_0007_01_000014 on host: cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 143. Diagnostics: [2025-12-09 15:30:36.777]Container killed on request. Exit code is 143\n","[2025-12-09 15:30:36.777]Container exited with a non-zero exit code 143. \n","[2025-12-09 15:30:36.778]Killed by external signal\n",".\n","25/12/09 15:30:38 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1765270135351_0007_01_000009 on host: cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 137. Diagnostics: [2025-12-09 15:30:38.042]Container killed on request. Exit code is 137\n","[2025-12-09 15:30:38.043]Container exited with a non-zero exit code 137. \n","[2025-12-09 15:30:38.043]Killed by external signal\n",".\n","25/12/09 15:30:38 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 9 on cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal: Container from a bad node: container_1765270135351_0007_01_000009 on host: cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 137. Diagnostics: [2025-12-09 15:30:38.042]Container killed on request. Exit code is 137\n","[2025-12-09 15:30:38.043]Container exited with a non-zero exit code 137. \n","[2025-12-09 15:30:38.043]Killed by external signal\n",".\n","25/12/09 15:30:38 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 9 for reason Container from a bad node: container_1765270135351_0007_01_000009 on host: cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 137. Diagnostics: [2025-12-09 15:30:38.042]Container killed on request. Exit code is 137\n","[2025-12-09 15:30:38.043]Container exited with a non-zero exit code 137. \n","[2025-12-09 15:30:38.043]Killed by external signal\n",".\n","25/12/09 15:30:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 354.0 (TID 6123) (cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765270135351_0007_01_000009 on host: cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 137. Diagnostics: [2025-12-09 15:30:38.042]Container killed on request. Exit code is 137\n","[2025-12-09 15:30:38.043]Container exited with a non-zero exit code 137. \n","[2025-12-09 15:30:38.043]Killed by external signal\n",".\n","25/12/09 15:30:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 354.0 (TID 6119) (cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765270135351_0007_01_000009 on host: cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal. Exit status: 137. Diagnostics: [2025-12-09 15:30:38.042]Container killed on request. Exit code is 137\n","[2025-12-09 15:30:38.043]Container exited with a non-zero exit code 137. \n","[2025-12-09 15:30:38.043]Killed by external signal\n",".\n","[Stage 590:==========================================>        (2056 + 8) / 2480]\r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)","Cell \u001B[0;32mIn[32], line 69\u001B[0m\n\u001B[1;32m     54\u001B[0m     new_ranks_partial \u001B[38;5;241m=\u001B[39m contribs_sum\u001B[38;5;241m.\u001B[39mmapValues(\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m contrib: base \u001B[38;5;241m+\u001B[39m d \u001B[38;5;241m*\u001B[39m contrib \u001B[38;5;241m+\u001B[39m dangling_contrib\n\u001B[1;32m     56\u001B[0m     )\n\u001B[1;32m     58\u001B[0m     ranks \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     59\u001B[0m         vertices\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m v: (v[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0.0\u001B[39m))          \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m     66\u001B[0m     )\n\u001B[0;32m---> 69\u001B[0m pr_bonus_df \u001B[38;5;241m=\u001B[39m \u001B[43mranks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoDF\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpagerank\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m pr_bonus_df \u001B[38;5;241m=\u001B[39m pr_bonus_df\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpagerank\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\n\u001B[1;32m     72\u001B[0m pr_bonus_df\u001B[38;5;241m.\u001B[39mrepartition(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgs://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbucket_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/pr_bonus\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     74\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m )\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/session.py:66\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=1)]\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/session.py:675\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pandas\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m    673\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(\n\u001B[1;32m    674\u001B[0m         data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 675\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/session.py:698\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    695\u001B[0m     prepare \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m obj: obj\n\u001B[1;32m    697\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[0;32m--> 698\u001B[0m     rdd, schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    700\u001B[0m     rdd, schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/session.py:486\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 486\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    487\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    488\u001B[0m     rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/session.py:460\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_inferSchema\u001B[39m(\u001B[38;5;28mself\u001B[39m, rdd, samplingRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    445\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001B[39;00m\n\u001B[1;32m    447\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001B[39;00m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 460\u001B[0m     first \u001B[38;5;241m=\u001B[39m \u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfirst\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m first:\n\u001B[1;32m    462\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe first row in RDD is empty, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    463\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py:1586\u001B[0m, in \u001B[0;36mRDD.first\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1573\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfirst\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1574\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1575\u001B[0m \u001B[38;5;124;03m    Return the first element in this RDD.\u001B[39;00m\n\u001B[1;32m   1576\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1584\u001B[0m \u001B[38;5;124;03m    ValueError: RDD is empty\u001B[39;00m\n\u001B[1;32m   1585\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1586\u001B[0m     rs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rs:\n\u001B[1;32m   1588\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m rs[\u001B[38;5;241m0\u001B[39m]\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py:1566\u001B[0m, in \u001B[0;36mRDD.take\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1563\u001B[0m         taken \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1565\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(partsScanned, \u001B[38;5;28mmin\u001B[39m(partsScanned \u001B[38;5;241m+\u001B[39m numPartsToTry, totalParts))\n\u001B[0;32m-> 1566\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunJob\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtakeUpToNumLeft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1568\u001B[0m items \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m res\n\u001B[1;32m   1569\u001B[0m partsScanned \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m numPartsToTry\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/context.py:1233\u001B[0m, in \u001B[0;36mSparkContext.runJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;66;03m# Implementation note: This is implemented as a mapPartitions followed\u001B[39;00m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;66;03m# by runJob() in order to avoid having to pass a Python lambda into\u001B[39;00m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;66;03m# SparkContext#runJob.\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m mappedRDD \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmapPartitions(partitionFunc)\n\u001B[0;32m-> 1233\u001B[0m sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunJob\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmappedRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartitions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, mappedRDD\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1296\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1305\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1031\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1033\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1035\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001B[0m, in \u001B[0;36mGatewayConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m   1196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JNetworkError(\n\u001B[1;32m   1197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError while sending\u001B[39m\u001B[38;5;124m\"\u001B[39m, e, proto\u001B[38;5;241m.\u001B[39mERROR_ON_SEND)\n\u001B[1;32m   1199\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1200\u001B[0m     answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m   1201\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m   1202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m answer\u001B[38;5;241m.\u001B[39mstartswith(proto\u001B[38;5;241m.\u001B[39mRETURN_MESSAGE):\n","File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n","\u001B[0;31mKeyboardInterrupt\u001B[0m: "]}],"source":["#If you have decided to do the bonus task - please copy the code here \n","\n","bonus_flag = False # Turn flag on (True) if you have implemented this part\n","\n","t_start = time()\n","\n","pages_links = (\n","    spark.read.parquet(f\"gs://{bucket_name}/multistream*\")\n","         .select(\"id\", \"anchor_text\")\n","         .rdd\n",")\n","edges = edges.distinct().cache()          \n","vertices = vertices.distinct().cache()    \n","\n","links = (\n","    edges\n","    .groupByKey()\n","    .mapValues(list)   \n","    .cache()\n",")\n","\n","num_vertices = vertices.count()\n","\n","ranks = vertices.map(lambda v: (v[0], 1.0 / num_vertices)).cache()\n","\n","d = 0.85  \n","\n","for it in range(10):\n","\n","    contribs = (\n","        links\n","        .join(ranks)  \n","        .flatMap(\n","            lambda node_links_rank: [\n","                (dst, node_links_rank[1][1] / len(node_links_rank[1][0]))\n","                for dst in node_links_rank[1][0]\n","                if len(node_links_rank[1][0]) > 0\n","            ]\n","        )\n","    )\n","\n","    contribs_sum = contribs.reduceByKey(lambda x, y: x + y)\n","\n","    dangling_mass = (\n","        ranks\n","        .subtractByKey(links)  \n","        .values()\n","        .sum()\n","    )\n","\n","    base = (1.0 - d) / num_vertices\n","    dangling_contrib = d * dangling_mass / num_vertices\n","\n","    new_ranks_partial = contribs_sum.mapValues(\n","        lambda contrib: base + d * contrib + dangling_contrib\n","    )\n","\n","    ranks = (\n","        vertices\n","        .map(lambda v: (v[0], 0.0))          \n","        .leftOuterJoin(new_ranks_partial)    \n","        .mapValues(\n","            lambda x: x[1] if x[1] is not None else base + dangling_contrib\n","        )\n","        .cache()\n","    )\n","\n","\n","pr_bonus_df = ranks.toDF([\"id\", \"pagerank\"])\n","pr_bonus_df = pr_bonus_df.orderBy(col(\"pagerank\").desc())\n","\n","pr_bonus_df.repartition(1).write.csv(\n","    f\"gs://{bucket_name}/pr_bonus\",\n","    compression=\"gzip\"\n",")\n","\n","pr_time_Bonus = time() - t_start\n"]},{"cell_type":"code","execution_count":33,"id":"855f9c94","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-PageRank_Bonus-time","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 590:==========================================>        (2074 + 8) / 2480]\r"]},{"ename":"NameError","evalue":"name 'pr_time_Bonus' is not defined","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","Cell \u001B[0;32mIn[33], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Note:test that PageRank computaion took less than 1 hour\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[43mpr_time_Bonus\u001B[49m \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m60\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m60\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m bonus_flag\n","\u001B[0;31mNameError\u001B[0m: name 'pr_time_Bonus' is not defined"]},{"name":"stderr","output_type":"stream","text":["25/12/09 16:33:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2200.0 in stage 590.0 (TID 72926) (cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal executor 12): TaskKilled (Stage cancelled)\n","25/12/09 16:33:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2201.0 in stage 590.0 (TID 72927) (cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal executor 13): TaskKilled (Stage cancelled)\n","25/12/09 16:33:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2203.0 in stage 590.0 (TID 72929) (cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal executor 13): TaskKilled (Stage cancelled)\n","25/12/09 16:33:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2204.0 in stage 590.0 (TID 72930) (cluster-0016-w-1.us-central1-a.c.ferrous-module-480122-e7.internal executor 15): TaskKilled (Stage cancelled)\n","25/12/09 16:33:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2202.0 in stage 590.0 (TID 72928) (cluster-0016-w-0.us-central1-a.c.ferrous-module-480122-e7.internal executor 16): TaskKilled (Stage cancelled)\n"]}],"source":["# Note:test that PageRank computaion took less than 1 hour\n","assert pr_time_Bonus < 60*60 and bonus_flag"]},{"cell_type":"code","execution_count":null,"id":"3d70069d","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}